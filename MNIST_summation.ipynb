{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b126ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torch import optim\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from transformers import get_scheduler\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499cf19",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "All four  \"training_\"  files contain the same data. They consist of sets of two 'MNIST' number pairs and correspoding sums of those two numbers.\n",
    "The four files are saved with different file extensions (.pt vs .pkl) or in different data types (dict vs tuple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad1f8b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/MNIST_pair/training_tuple.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "with open('./data/MNIST_pair/training_dict.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data = torch.load(\"./data/MNIST_pair/training_dict.pt\")\n",
    "\n",
    "data = torch.load(\"./data/MNIST_pair/training_tuple.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d2005a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/MNIST_pair/test.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "test_data = torch.load(\"./data/MNIST_pair/test.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b2be85",
   "metadata": {},
   "source": [
    "## Prepare Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6d671",
   "metadata": {},
   "source": [
    "Split data into train data and validation data (if you need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9ea1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_len = int(data[0].size(0) * 0.75)\n",
    "\n",
    "perm_idxs = torch.randperm(data[0].size(0))\n",
    "\n",
    "train_idx = perm_idxs[:tr_len]\n",
    "valid_idx = perm_idxs[tr_len: ]\n",
    "\n",
    "train_data = data[0][train_idx]\n",
    "train_label = data[1][train_idx]\n",
    "assert len(train_data) == len(train_label)\n",
    "\n",
    "valid_data = data[0][valid_idx]\n",
    "valid_label = data[1][valid_idx]\n",
    "assert len(valid_data) == len(valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c7c944",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = train_data.float().mean(), train_data.float().std()\n",
    "\n",
    "train_data = ((train_data - mean) / std, train_label)\n",
    "valid_data = ((valid_data - mean) / std , valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7624347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, dataset, test:bool = False):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        \n",
    "        if test:\n",
    "            self.num1 = dataset[0][:, None, ]\n",
    "            self.num2 = dataset[0][:, None, ]\n",
    "            self.labels = dataset[1]\n",
    "        \n",
    "        else:\n",
    "            num_pairs = dataset[0]\n",
    "            self.labels = dataset[1]\n",
    "            self.num1 = num_pairs[:, 0, None, ]\n",
    "            self.num2 = num_pairs[:, 1, None, ]\n",
    "            assert len(self.num1) == len(self.num2)\n",
    "                     \n",
    "        assert len(self.num1) == len(self.labels)\n",
    "        \n",
    "    def __len__(self,):\n",
    "        return len(self.num1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.num1[idx].float(), self.num2[idx].float(), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d91792e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[39m=\u001b[39m MNISTDataset(train_data, test\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m val_dataset \u001b[39m=\u001b[39m MNISTDataset(valid_data, test\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m test_dataset \u001b[39m=\u001b[39m MNISTDataset(test_data, test\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = MNISTDataset(train_data, test=False)\n",
    "val_dataset = MNISTDataset(valid_data, test=False)\n",
    "test_dataset = MNISTDataset(test_data, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361a66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b96ae",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ad08610",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_method = \"combination\"\n",
    "if predict_method == \"combination\":\n",
    "    \n",
    "    \"\"\" look up table for all permutations corresponding to summation\"\"\"\n",
    "    import itertools\n",
    "    from copy import deepcopy\n",
    "    from collections import defaultdict\n",
    "\n",
    "    num = (train_data[1].unique() / 2).int().unique().numpy().tolist()\n",
    "    assert len(num) == 10\n",
    "    num = num * 2\n",
    "    \n",
    "    per_list = sorted((set(itertools.permutations(num, 2, ))), key=lambda x: x)\n",
    "   \n",
    "    assert len(per_list) == 100\n",
    "\n",
    "    label_dic = defaultdict(list)\n",
    "    \n",
    "    for i in per_list:\n",
    "        label_dic[sum(i)].append(i)\n",
    "\n",
    "    label_dic = pd.DataFrame(data=label_dic.values(), index=label_dic.keys()).T\n",
    "\n",
    "    assert 100 == (~label_dic.isna()).sum().sum()\n",
    "    \n",
    "    def custom_loss(pred, label):\n",
    "        \n",
    "        return nn.NLLLoss(reduction=\"none\")(nn.LogSoftmax(dim=-1)(pred), label)\n",
    "    \n",
    "    loss_function = custom_loss\n",
    "    \n",
    "else: \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9434d2",
   "metadata": {},
   "source": [
    "# ==========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d99fd5",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26fa37",
   "metadata": {},
   "source": [
    "# Prepare Data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3da172f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, dataset, test:bool = False):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        \n",
    "        if test:\n",
    "            self.num1 = dataset[0][:, None, ]\n",
    "            self.num2 = dataset[0][:, None, ]\n",
    "            self.labels = dataset[1]\n",
    "        \n",
    "        else:\n",
    "            num_pairs = dataset[0]\n",
    "            self.labels = dataset[1]\n",
    "            self.num1 = num_pairs[:, 0, None, ]\n",
    "            self.num2 = num_pairs[:, 1, None, ]\n",
    "            assert len(self.num1) == len(self.num2)\n",
    "                     \n",
    "        assert len(self.num1) == len(self.labels)\n",
    "        \n",
    "    def __len__(self,):\n",
    "        return len(self.num1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.num1[idx].float(), self.num2[idx].float(), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3da2d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(batch_size):\n",
    "    \"\"\"Data For Train and Validation\"\"\"\n",
    "    with open('./data/MNIST_pair/training_tuple.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    \"\"\"Split Train and Valdiation set\"\"\"\n",
    "    tr_len = int(data[0].size(0) * 0.75)\n",
    "\n",
    "    perm_idxs = torch.randperm(data[0].size(0))\n",
    "\n",
    "    train_idx = perm_idxs[:tr_len]\n",
    "    valid_idx = perm_idxs[tr_len: ]\n",
    "\n",
    "    train_data = data[0][train_idx]\n",
    "    train_label = data[1][train_idx]\n",
    "    assert len(train_data) == len(train_label)\n",
    "\n",
    "    valid_data = data[0][valid_idx]\n",
    "    valid_label = data[1][valid_idx]\n",
    "    assert len(valid_data) == len(valid_label)\n",
    "        \n",
    "    \"\"\"Normalize\"\"\"\n",
    "    mean, std = train_data.float().mean(), train_data.float().std()\n",
    "    train_data = ((train_data - mean) / std, train_label)\n",
    "    valid_data = ((valid_data - mean) / std , valid_label)\n",
    "\n",
    "#     train_data = (train_data, train_label)\n",
    "#     valid_data = (valid_data, valid_label)\n",
    "    \n",
    "\n",
    "    \"\"\"Data For Test\"\"\"\n",
    "    with open('./data/MNIST_pair/test.pkl', 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    test_label = test_data[1]\n",
    "    \n",
    "    \"\"\"Normalize\"\"\"\n",
    "    mean, std = test_data[0].float().mean(), test_data[0].float().std()\n",
    "    test_data = ((test_data[0] - mean) / std, test_label)\n",
    "\n",
    "#     test_data = (test_data[0], test_label)\n",
    "\n",
    "    \n",
    "    \"\"\"Make Dataset\"\"\"\n",
    "    train_dataset = MNISTDataset(train_data, test=False)\n",
    "    val_dataset = MNISTDataset(valid_data, test=False)\n",
    "    test_dataset = MNISTDataset(test_data, test=True)\n",
    "    \n",
    "    \"\"\"Make DataLoader\"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11edeb8b",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afed0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, target):\n",
    "    pred_num = torch.argmax(pred, dim=-1)\n",
    "    \n",
    "    wandb.log({\"acc/pred\": pred_num, \n",
    "               \"acc/label\": target,\n",
    "               \"acc/acc\": (pred_num == target).float().mean()})\n",
    "    \n",
    "    return (pred_num == target).float().mean()\n",
    "\n",
    "def accuracy_sum(pred1,pred2, target):\n",
    "    pred_num1 = torch.argmax(pred1, dim=-1)\n",
    "    pred_num2 = torch.argmax(pred2,dim=-1)\n",
    "    pred_sum = pred_num1 + pred_num2\n",
    "    return (pred_sum == target).float().mean()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf10549",
   "metadata": {},
   "source": [
    "# Prepare loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da2dbef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_function(predict_method):\n",
    "    if predict_method == \"inverse_augment\":\n",
    "        \"\"\"  Make label dictionary storing all permutations corresponding to summation\"\"\"\n",
    "\n",
    "        num = list(range(10))   \n",
    "        num = num * 2\n",
    "        per_list = sorted((set(itertools.permutations(num, 2, ))), key=lambda x: x)\n",
    "\n",
    "        assert len(per_list) == 100\n",
    "\n",
    "        label_dic = defaultdict(list)\n",
    "        for i in per_list:\n",
    "            label_dic[sum(i)].append(i)\n",
    "\n",
    "        label_dic = pd.DataFrame(data=label_dic.values(), index=label_dic.keys()).T\n",
    "\n",
    "        assert 100 == (~label_dic.isna()).sum().sum()\n",
    "\n",
    "        def custom_loss(pred, label):\n",
    "\n",
    "            return nn.NLLLoss(reduction=\"none\")(nn.LogSoftmax(dim=-1)(pred), label)\n",
    "\n",
    "        loss_function = custom_loss\n",
    "        \n",
    "        return loss_function, label_dic\n",
    "    else: \n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "    \n",
    "        return loss_function, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d952c1db",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d509d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2), # 6 24 24 -> 6 12 12\n",
    "            nn.Conv2d(6, 16, 5), # 6 12 12 -> 16 8 8\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2), # 16 8 8 -> 16 4 4\n",
    "        )\n",
    "        \n",
    "        for m in self.encoder:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab0c09e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, N=10):\n",
    "        super().__init__()\n",
    "        self.classifier =  nn.Sequential(\n",
    "            nn.Linear(16 * 4 * 4, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, N)\n",
    "        )\n",
    "        \n",
    "        for m in self.classifier:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd5c1c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(predict_method):\n",
    "    encoder = MNIST_Encoder()\n",
    "    \n",
    "    if predict_method in [\"inverse_augment\", \"combination\"]:\n",
    "        classifier = Classifier(N=10)\n",
    "    \n",
    "    elif predict_method in ['add_hs', 'add_logits']:\n",
    "        classifier = Classifier(N=19)\n",
    "    else:\n",
    "        raise ValueError (f\"Method only have 3 options : ['add_hs', 'add_logits', 'combination', 'inverse_augment'], but {predict_method} is given\")\n",
    "    return encoder, classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbfb4ee",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c9d13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_acc(h1, h2, label, classifier, loss_function, label_dic, predict_method: str = \"add_hs\", test=False):\n",
    "    if predict_method == \"combination\":\n",
    "        if test:\n",
    "            logit = classifier(h1)\n",
    "\n",
    "            loss = loss_function(logit, label).mean()\n",
    "\n",
    "            acc = accuracy(logit, label)\n",
    "\n",
    "            return loss, acc    \n",
    "            \n",
    "        \n",
    "        logit1 = classifier(h1)\n",
    "        logit2 = classifier(h2)\n",
    "        \n",
    "        prob1 = nn.Softmax(dim=-1)(logit1).unsqueeze(-1)\n",
    "        prob2 = nn.Softmax(dim=-1)(logit2).unsqueeze(1)\n",
    "        \n",
    "        batch_matrix = prob1 @ prob2\n",
    "    \n",
    "        anti_diag = [\n",
    "            torch.stack(\n",
    "                        [torch.sum(torch.diag(torch.fliplr(mat), diag)) \n",
    "                         for diag in range(len(mat)-1, -len(mat), -1)\n",
    "                        ]\n",
    "                        ) \n",
    "                for mat in batch_matrix ]\n",
    "        \n",
    "        batch_preds = torch.stack(anti_diag)\n",
    "        \n",
    "        loss = loss_function(batch_preds, label)\n",
    "        \n",
    "        acc = accuracy(batch_preds, label)\n",
    "        \n",
    "        return loss, acc\n",
    "        \n",
    "    elif predict_method == \"add_hs\":\n",
    "        if test:\n",
    "            label = label *2\n",
    "        \n",
    "        hidden = (h1 + h2) / 2\n",
    "        \n",
    "        logit = classifier(hidden)\n",
    "        \n",
    "        loss = loss_function(logit, label)\n",
    "\n",
    "        acc = accuracy(logit, label)        \n",
    "    \n",
    "        return loss, acc\n",
    "    \n",
    "    elif predict_method == \"add_logits\":\n",
    "        if test:\n",
    "            label = label *2\n",
    "        \n",
    "        logit1 = classifier(h1)\n",
    "        logit2 = classifier(h2)\n",
    "        \n",
    "        logit = (logit1 + logit2) / 2\n",
    "        \n",
    "        loss = loss_function(logit, label)\n",
    "\n",
    "        acc = accuracy(logit, label)        \n",
    "\n",
    "        return loss, acc\n",
    "        \n",
    "    elif predict_method == \"inverse_augment\":\n",
    "        \n",
    "        if test:\n",
    "            logit = classifier(h1)\n",
    "            \n",
    "            loss = loss_function(logit, label).mean()\n",
    "            \n",
    "            acc = accuracy(logit, label)\n",
    "            \n",
    "            return loss, acc\n",
    "        \n",
    "        logit1 = classifier(h1)\n",
    "        logit2 = classifier(h2)\n",
    "        \n",
    "        loss, acc = 0, 0\n",
    "        \n",
    "        assert label_dic is not None\n",
    "        \n",
    "        for lo1, lo2, (i, inverse_augments) in zip(logit1, logit2, label_dic[label].items()):\n",
    "\n",
    "            label1 = torch.tensor(list(zip(*inverse_augments.dropna().values))[0])\n",
    "            label2 = torch.tensor(list(zip(*inverse_augments.dropna().values))[1])\n",
    "            \n",
    "            lo1 = lo1.expand(label1.shape[0], -1)\n",
    "            lo2 = lo2.expand(label2.shape[0], -1)\n",
    "            \n",
    "            loss += (loss_function(lo1, label1) + loss_function(lo2, label2)).mean()\n",
    "            \n",
    "            acc += (accuracy(lo1, label1) + accuracy(lo2, label2)).mean()\n",
    "            \n",
    "        loss /= len(label)\n",
    "        acc /= len(label)\n",
    "        \n",
    "        return loss, acc\n",
    "        \n",
    "    else:\n",
    "        raise ValueError (f\"Method only have 3 options : ['add_hs', 'add_logits', 'inverse_augment'], but {predict_method} is given\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e7f1bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch: int, \n",
    "                encoder: nn.Module,\n",
    "                classifier:nn.Module,\n",
    "                dataloader:torch.utils.data.DataLoader, \n",
    "                loss_function: nn.Module, \n",
    "                optimizer: torch.optim.Optimizer, \n",
    "                lr_scheduler,\n",
    "                predict_method: str,\n",
    "                label_dic = None,\n",
    "               ):\n",
    "    \n",
    "    wandb.define_metric(\"Train/step\")\n",
    "    wandb.define_metric(\"Train/*\", step_metric=\"Train/step\")\n",
    "    \n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "    \n",
    "    with tqdm(enumerate(dataloader), desc=f\"Training Epoch {epoch}\", total=len(dataloader)) as train_bar:\n",
    "        for tri, batch  in train_bar:\n",
    "            \n",
    "            encoder.train()\n",
    "            classifier.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            num1, num2, label = batch\n",
    "\n",
    "            h1 = encoder(num1)\n",
    "            h2 = encoder(num2)\n",
    "\n",
    "            tr_loss, tr_acc = calculate_loss_acc(h1, h2, label, \n",
    "                                                classifier, \n",
    "                                                loss_function, \n",
    "                                                label_dic, \n",
    "                                                predict_method)\n",
    "            \n",
    "            tr_loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += tr_loss.item()\n",
    "            total_acc += tr_acc.item()\n",
    "            \n",
    "            train_bar.set_description(f\"Train Step {tri} || Train ACC {tr_acc: .4f} | Train Loss {tr_loss.item(): .4f}\")\n",
    "            \n",
    "            log_dict = {\"Train/step\": tri + epoch*len(dataloader),\n",
    "                        \"Train/Accuracy\": tr_acc,\n",
    "                        \"Train/Loss\": tr_loss}\n",
    "                \n",
    "            wandb.log(log_dict)\n",
    "            \n",
    "    return total_loss/len(dataloader), total_acc / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a8f791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(epoch: int, \n",
    "                encoder: nn.Module,\n",
    "                classifier:nn.Module,\n",
    "                dataloader:torch.utils.data.DataLoader, \n",
    "                loss_function: nn.Module, \n",
    "                predict_method: str,\n",
    "                label_dic = None                \n",
    "               ):\n",
    "    \n",
    "    wandb.define_metric(\"Valid/step\")\n",
    "    wandb.define_metric(\"Valid/*\", step_metric=\"Valid/step\")\n",
    "        \n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(enumerate(dataloader), desc=f\"Val Epoch {epoch}\", total=len(dataloader)) as val_bar:\n",
    "            for vli, batch  in val_bar:\n",
    "\n",
    "                num1, num2, label = batch\n",
    "\n",
    "                h1 = encoder(num1)\n",
    "                h2 = encoder(num2)\n",
    "\n",
    "                vl_loss, vl_acc = calculate_loss_acc(h1, h2, label,\n",
    "                                                    classifier, \n",
    "                                                    loss_function, \n",
    "                                                    label_dic, \n",
    "                                                    predict_method)\n",
    "\n",
    "                total_loss += vl_loss.item()\n",
    "                total_acc += vl_acc.item()\n",
    "\n",
    "                val_bar.set_description(f\"Val Step {vli} || Val ACC {vl_acc: .4f} | Val Loss {vl_loss: .4f}\")\n",
    "                \n",
    "                log_dict = {\"Valid/step\": vli + epoch*len(dataloader),\n",
    "                            \"Valid/Accuracy\": vl_acc,\n",
    "                            \"Valid/Loss\": vl_loss}\n",
    "                \n",
    "                wandb.log(log_dict)\n",
    "                \n",
    "    return total_loss/len(dataloader), total_acc / len(dataloader)               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7be44f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder: nn.Module,\n",
    "        classifier:nn.Module,\n",
    "        dataloader:torch.utils.data.DataLoader, \n",
    "        loss_function: nn.Module,\n",
    "        predict_method: str,\n",
    "        label_dic = None                \n",
    "    ):\n",
    "    \n",
    "    wandb.define_metric(\"Test/step\")\n",
    "    wandb.define_metric(\"Test/*\", step_metric=\"Test/step\")\n",
    "        \n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(enumerate(dataloader), desc=\"Test\", total=len(dataloader)) as test_bar:\n",
    "            for tti, batch  in test_bar:\n",
    "\n",
    "                num, _, label = batch\n",
    "\n",
    "                hs = encoder(num)\n",
    "                \n",
    "                tt_loss, tt_acc = calculate_loss_acc(hs, hs, label,\n",
    "                                                    classifier, \n",
    "                                                    loss_function, \n",
    "                                                    label_dic, \n",
    "                                                    predict_method,\n",
    "                                                    True)\n",
    "\n",
    "                total_loss += tt_loss.item()\n",
    "                total_acc += tt_acc.item()\n",
    "                \n",
    "                test_bar.set_description(f\"Test Step {tti} || Test ACC {tt_acc: .4f} | Test Loss {tt_loss: .4f}\")\n",
    "                \n",
    "                log_dict = {\"Test/step\": tti,\n",
    "                            \"Test/Accuracy\": tt_acc,\n",
    "                            \"Test/Loss\": tt_loss}\n",
    "                \n",
    "                wandb.log(log_dict)\n",
    "\n",
    "    return total_loss/len(dataloader), total_acc / len(dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ec8037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_epoch, batch_size, predict_method, lr, weight_decay):\n",
    "\n",
    "    \n",
    "    train_loader, valid_loader, test_loader = get_loader(batch_size)\n",
    "    \n",
    "    loss_function, label_dic = get_loss_function(predict_method)\n",
    "    \n",
    "    encoder, classifier = get_models(predict_method)\n",
    "\n",
    "    wandb.watch((encoder, classifier))\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(params=[{\"params\":encoder.parameters(), \"params\":classifier.parameters()}],\n",
    "                                           lr=lr, \n",
    "                                           weight_decay=weight_decay\n",
    "                                 )\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\"cosine\", optimizer=optimizer, \n",
    "                                 num_warmup_steps=int(len(train_loader)*num_epoch*0.1),\n",
    "                                 num_training_steps=len(train_loader)*num_epoch\n",
    "                                )\n",
    "                                           \n",
    "    with tqdm(range(num_epoch), desc=\"Total Epoch\", total=num_epoch) as total_bar:\n",
    "    \n",
    "        for epoch in total_bar:\n",
    "            \n",
    "            train_loss, train_acc = train_epoch(epoch, \n",
    "                                                encoder, \n",
    "                                                classifier, \n",
    "                                                train_loader, \n",
    "                                                loss_function, \n",
    "                                                optimizer, \n",
    "                                                lr_scheduler, \n",
    "                                                predict_method,\n",
    "                                                label_dic)\n",
    "            \n",
    "            valid_loss, valid_acc = valid_epoch(epoch, \n",
    "                                                encoder,\n",
    "                                                classifier,\n",
    "                                                valid_loader,\n",
    "                                                loss_function,\n",
    "                                                predict_method,\n",
    "                                                label_dic)\n",
    "            \n",
    "                            \n",
    "            total_bar.set_description(f\"Epoch {epoch} |||| Train ACC {train_acc:.4f} \\\n",
    "                                        Train Epoch Loss {train_loss:.4f} || \\\n",
    "                                        Valid Epoch ACC {valid_acc:.4f} \\\n",
    "                                        Valid Epoch Loss {valid_loss:.4f}\")\n",
    "            \n",
    "            wandb.log({\"Epoch/Epoch\": epoch,\n",
    "                       \"Total_ACC/Train Epoch ACC\": train_acc,\n",
    "                       \"Total_Loss/Train Epoch Loss\": train_loss,\n",
    "                       \"Total_ACC/Valid Epoch ACC \": valid_acc,\n",
    "                       \"Total_Loss/Valid Epoch Loss\": valid_loss,\n",
    "                        })\n",
    "\n",
    "        test_loss, test_acc = test(encoder,\n",
    "                                    classifier,\n",
    "                                    test_loader,\n",
    "                                    loss_function,\n",
    "                                    predict_method,\n",
    "                                    label_dic)\n",
    "        \n",
    "        wandb.log({\"Total_ACC/Test Accuracy\": test_acc,\n",
    "                    \"Total_Loss/Test Loss\": test_loss})\n",
    "    \n",
    "    torch.save(encoder.state_dict(), f'./result/{wandb.config.id}/')\n",
    "    torch.save(classifier.state_dict(), f'./result/{wandb.config.id}/')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35a2bcb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epoch = 2\n",
    "batch_size = 64\n",
    "# ['add_hs' or 'add_logits' or 'combination','inverse_augment']\n",
    "predict_method = ['add_hs', 'add_logits', 'combination', 'inverse_augment'][2]    \n",
    "lr = 0.0001\n",
    "weight_decay = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55cbfeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_epoch': 100, 'batch_size': 256, 'predict_method': 'combination', 'lr': 0.0005, 'weight_decay': 1e-05, 'id': 'combination0.19143462883965856'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:combination0.8435870402589428) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a23fd0a15fbf4064ad0340397df8eae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">combination0.8435870402589428</strong>: <a href=\"https://wandb.ai/hacastle12/MNIST%20addition/runs/combination0.8435870402589428\" target=\"_blank\">https://wandb.ai/hacastle12/MNIST%20addition/runs/combination0.8435870402589428</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230324_012653-combination0.8435870402589428/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:combination0.8435870402589428). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.14.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jylab_intern001/MNIST/wandb/run-20230324_012743-combination0.19143462883965856</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/hacastle12/MNIST%20addition/runs/combination0.19143462883965856\" target=\"_blank\">combination0.19143462883965856</a></strong> to <a href=\"https://wandb.ai/hacastle12/MNIST%20addition\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cef8f2ab9994365a57751ec2a912d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Total Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ea751fabc124cef90a8a6eeb1893ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 0:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b705424a70734996b6c6792240bff1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 0:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ca07433e314c6784506cc753452b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21df97e4f3764ca9a0f89dd452c9ef17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 1:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb6a9188c4c74094a0e407ada4bbe1d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94f50fa05574bb09208f2aae470995a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 2:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "206842c27c6d432faef4bd89f0913287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b72fea7e968442a9c8ea231aa1a129a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 3:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724c0a7caf6f470ea30e7e8f93ba3472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17811ffd741f4384b823b6fcff4005b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 4:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e39ce1a51564912996ea58ce7c12099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63526a9f8f4a4f36bdd324e8f65fc1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 5:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3897714d24754e5eb05ca23eb0167f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 6:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560b533f507a4e00ac4a546c459fb51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 6:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e26fd56ef741868d93a8c12322c0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 7:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "620c80eb67214fc2af6d9f4454064efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 7:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c45491ac4a14b4db32f9b0d71474d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 8:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da098504ae6a47f0949833bf353a08bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 8:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b152ec9563345bd9543dbc3f5032da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 9:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4013af97d27643e48fe584eac1fbfd6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 9:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45893364d80e4df5a54d2f75dd699566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 10:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09996d6f42f642538693b530cb64c9f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 10:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77079d33a03141c583efeeac2f67938b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 11:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f77c0bc662d438d889764ca371b8cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 11:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4c056f7499475fa4196c0b4236a4cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 12:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c791bdbaa1384c0c9877db1aa52d76b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 12:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748e8f035d204685b69e493417ea6272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 13:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a8aad7a8fe540f39032b60cad2dabc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 13:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d59b2b959974ee3807a88cd14ea00bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 14:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2296413570144c36810263d19d08196b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 14:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b014ea78eaeb4d47a44462053406153f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 15:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10e67bbdd7324f7a8da63d36b7d491b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 15:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c9025a8dbd4e139767f707107825c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 16:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a1bfaacaa84c8fb8af3c25bd80720f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 16:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f11eaeb847417dad2e6e9dd87fb877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 17:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3c63eccc5f49a38dee1de5ca962fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 17:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afba095a7c6341369f07a1b0e20b5e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 18:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5410b0e22ef4495986a52da344ec076a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 18:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29cd86d16764ef48cdbced1bcb28a0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 19:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd37de26be046999f8e5a9e28ebc01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 19:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ffde92c9ae4f8ab5a18d4eb5003d5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 20:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8de8c24ee04ec7812bd7084aed51c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 20:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b5b287f19f420ab7a336fa45d91c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 21:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35b2c95cde0b4ab1883a57793dceccd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Val Epoch 21:   0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "621ac804fb6842a89434ca3882a850d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 22:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 27\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./result/\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mid\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/config.json\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     24\u001b[0m     json\u001b[39m.\u001b[39mdump(config, f)\n\u001b[0;32m---> 27\u001b[0m main(num_epoch\u001b[39m=\u001b[39;49mnum_epoch, batch_size\u001b[39m=\u001b[39;49mbatch_size, predict_method\u001b[39m=\u001b[39;49mpredict_method, lr\u001b[39m=\u001b[39;49mlr, weight_decay\u001b[39m=\u001b[39;49mweight_decay)\n",
      "Cell \u001b[0;32mIn[34], line 26\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(num_epoch, batch_size, predict_method, lr, weight_decay)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(num_epoch), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTotal Epoch\u001b[39m\u001b[39m\"\u001b[39m, total\u001b[39m=\u001b[39mnum_epoch) \u001b[39mas\u001b[39;00m total_bar:\n\u001b[1;32m     24\u001b[0m     \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m total_bar:\n\u001b[0;32m---> 26\u001b[0m         train_loss, train_acc \u001b[39m=\u001b[39m train_epoch(epoch, \n\u001b[1;32m     27\u001b[0m                                             encoder, \n\u001b[1;32m     28\u001b[0m                                             classifier, \n\u001b[1;32m     29\u001b[0m                                             train_loader, \n\u001b[1;32m     30\u001b[0m                                             loss_function, \n\u001b[1;32m     31\u001b[0m                                             optimizer, \n\u001b[1;32m     32\u001b[0m                                             lr_scheduler, \n\u001b[1;32m     33\u001b[0m                                             predict_method,\n\u001b[1;32m     34\u001b[0m                                             label_dic)\n\u001b[1;32m     36\u001b[0m         valid_loss, valid_acc \u001b[39m=\u001b[39m valid_epoch(epoch, \n\u001b[1;32m     37\u001b[0m                                             encoder,\n\u001b[1;32m     38\u001b[0m                                             classifier,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m                                             predict_method,\n\u001b[1;32m     42\u001b[0m                                             label_dic)\n\u001b[1;32m     45\u001b[0m         total_bar\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m |||| Train ACC \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m                                    Train Epoch Loss \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m || \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m                                    Valid Epoch ACC \u001b[39m\u001b[39m{\u001b[39;00mvalid_acc\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m                                    Valid Epoch Loss \u001b[39m\u001b[39m{\u001b[39;00mvalid_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 35\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(epoch, encoder, classifier, dataloader, loss_function, optimizer, lr_scheduler, predict_method, label_dic)\u001b[0m\n\u001b[1;32m     27\u001b[0m h2 \u001b[39m=\u001b[39m encoder(num2)\n\u001b[1;32m     29\u001b[0m tr_loss, tr_acc \u001b[39m=\u001b[39m calculate_loss_acc(h1, h2, label, \n\u001b[1;32m     30\u001b[0m                                     classifier, \n\u001b[1;32m     31\u001b[0m                                     loss_function, \n\u001b[1;32m     32\u001b[0m                                     label_dic, \n\u001b[1;32m     33\u001b[0m                                     predict_method)\n\u001b[0;32m---> 35\u001b[0m tr_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     36\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     37\u001b[0m lr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/seal_ad/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/seal_ad/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/seal_ad/lib/python3.8/site-packages/wandb/wandb_torch.py:264\u001b[0m, in \u001b[0;36mTorchHistory._hook_variable_gradient_stats.<locals>.<lambda>\u001b[0;34m(grad)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog_tensor_stats(grad\u001b[39m.\u001b[39mdata, name)\n\u001b[0;32m--> 264\u001b[0m handle \u001b[39m=\u001b[39m var\u001b[39m.\u001b[39mregister_hook(\u001b[39mlambda\u001b[39;00m grad: _callback(grad, log_track))\n\u001b[1;32m    265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_handles[name] \u001b[39m=\u001b[39m handle\n\u001b[1;32m    266\u001b[0m \u001b[39mreturn\u001b[39;00m handle\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import json\n",
    "    import os\n",
    "    from random import random\n",
    "    \n",
    "    with open('./mnist/config.json') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    num_epoch = config.get('num_epoch')\n",
    "    batch_size = config.get('batch_size')\n",
    "    predict_method = config.get('predict_method')    # ['add_hs' or 'add_logits' or 'combination', 'inverse_augment']\n",
    "    lr =config.get('lr')\n",
    "    weight_decay = config.get('weight_decay')\n",
    "    \n",
    "    id = predict_method+str(random())\n",
    "    config['id'] = id\n",
    "    print(config)\n",
    "\n",
    "    wandb.init(project=\"MNIST addition\", config=config, id=id)\n",
    "\n",
    "    os.makedirs(f\"./result/{id}/\")\n",
    "\n",
    "    with open(f'./result/{id}/config.json', 'w') as f:\n",
    "        json.dump(config, f)\n",
    "\n",
    "\n",
    "    main(num_epoch=num_epoch, batch_size=batch_size, predict_method=predict_method, lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad5c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
