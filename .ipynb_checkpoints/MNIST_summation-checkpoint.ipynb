{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b126ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import pickle\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torch import optim\n",
    "from datetime import datetime\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "from transformers import get_scheduler\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499cf19",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "All four  \"training_\"  files contain the same data. They consist of sets of two 'MNIST' number pairs and correspoding sums of those two numbers.\n",
    "The four files are saved with different file extensions (.pt vs .pkl) or in different data types (dict vs tuple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad1f8b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/MNIST_pair/training_tuple.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "with open('./data/MNIST_pair/training_dict.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "data = torch.load(\"./data/MNIST_pair/training_dict.pt\")\n",
    "\n",
    "data = torch.load(\"./data/MNIST_pair/training_tuple.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d2005a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/MNIST_pair/test.pkl', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "test_data = torch.load(\"./data/MNIST_pair/test.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b2be85",
   "metadata": {},
   "source": [
    "## Prepare Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b6d671",
   "metadata": {},
   "source": [
    "Split data into train data and validation data (if you need)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9ea1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_len = int(data[0].size(0) * 0.75)\n",
    "\n",
    "perm_idxs = torch.randperm(data[0].size(0))\n",
    "\n",
    "train_idx = perm_idxs[:tr_len]\n",
    "valid_idx = perm_idxs[tr_len: ]\n",
    "\n",
    "train_data = data[0][train_idx]\n",
    "train_label = data[1][train_idx]\n",
    "assert len(train_data) == len(train_label)\n",
    "\n",
    "valid_data = data[0][valid_idx]\n",
    "valid_label = data[1][valid_idx]\n",
    "assert len(valid_data) == len(valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6c7c944",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean, std = train_data.float().mean(), train_data.float().std()\n",
    "\n",
    "train_data = ((train_data - mean) / std, train_label)\n",
    "valid_data = ((valid_data - mean) / std , valid_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7624347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, dataset, test:bool = False):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        \n",
    "        if test:\n",
    "            self.num1 = dataset[0][:, None, ]\n",
    "            self.num2 = dataset[0][:, None, ]\n",
    "            self.labels = dataset[1]\n",
    "        \n",
    "        else:\n",
    "            num_pairs = dataset[0]\n",
    "            self.labels = dataset[1]\n",
    "            self.num1 = num_pairs[:, 0, None, ]\n",
    "            self.num2 = num_pairs[:, 1, None, ]\n",
    "            assert len(self.num1) == len(self.num2)\n",
    "                     \n",
    "        assert len(self.num1) == len(self.labels)\n",
    "        \n",
    "    def __len__(self,):\n",
    "        return len(self.num1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.num1[idx].float(), self.num2[idx].float(), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d91792e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNISTDataset(train_data, test=False)\n",
    "val_dataset = MNISTDataset(valid_data, test=False)\n",
    "test_dataset = MNISTDataset(test_data, test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "361a66a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b96ae",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ad08610",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_method = \"combination\"\n",
    "if predict_method == \"combination\":\n",
    "    \n",
    "    \"\"\" look up table for all permutations corresponding to summation\"\"\"\n",
    "    import itertools\n",
    "    from copy import deepcopy\n",
    "    from collections import defaultdict\n",
    "\n",
    "    num = (train_data[1].unique() / 2).int().unique().numpy().tolist()\n",
    "    assert len(num) == 10\n",
    "    num = num * 2\n",
    "    \n",
    "    per_list = sorted((set(itertools.permutations(num, 2, ))), key=lambda x: x)\n",
    "   \n",
    "    assert len(per_list) == 100\n",
    "\n",
    "    label_dic = defaultdict(list)\n",
    "    \n",
    "    for i in per_list:\n",
    "        label_dic[sum(i)].append(i)\n",
    "\n",
    "    label_dic = pd.DataFrame(data=label_dic.values(), index=label_dic.keys()).T\n",
    "\n",
    "    assert 100 == (~label_dic.isna()).sum().sum()\n",
    "    \n",
    "    def custom_loss(pred, label):\n",
    "        \n",
    "        return nn.NLLLoss(reduction=\"none\")(nn.LogSoftmax(dim=-1)(pred), label)\n",
    "    \n",
    "    loss_function = custom_loss\n",
    "    \n",
    "else: \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9434d2",
   "metadata": {},
   "source": [
    "# ==========================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d99fd5",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26fa37",
   "metadata": {},
   "source": [
    "# Prepare Data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3da172f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, dataset, test:bool = False):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        \n",
    "        \n",
    "        if test:\n",
    "            self.num1 = dataset[0][:, None, ]\n",
    "            self.num2 = dataset[0][:, None, ]\n",
    "            self.labels = dataset[1]\n",
    "        \n",
    "        else:\n",
    "            num_pairs = dataset[0]\n",
    "            self.labels = dataset[1]\n",
    "            self.num1 = num_pairs[:, 0, None, ]\n",
    "            self.num2 = num_pairs[:, 1, None, ]\n",
    "            assert len(self.num1) == len(self.num2)\n",
    "                     \n",
    "        assert len(self.num1) == len(self.labels)\n",
    "        \n",
    "    def __len__(self,):\n",
    "        return len(self.num1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.num1[idx].float(), self.num2[idx].float(), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3da2d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(batch_size):\n",
    "    \"\"\"Data For Train and Validation\"\"\"\n",
    "    with open('./data/MNIST_pair/training_tuple.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    \"\"\"Split Train and Valdiation set\"\"\"\n",
    "    tr_len = int(data[0].size(0) * 0.75)\n",
    "\n",
    "    perm_idxs = torch.randperm(data[0].size(0))\n",
    "\n",
    "    train_idx = perm_idxs[:tr_len]\n",
    "    valid_idx = perm_idxs[tr_len: ]\n",
    "\n",
    "    train_data = data[0][train_idx]\n",
    "    train_label = data[1][train_idx]\n",
    "    assert len(train_data) == len(train_label)\n",
    "\n",
    "    valid_data = data[0][valid_idx]\n",
    "    valid_label = data[1][valid_idx]\n",
    "    assert len(valid_data) == len(valid_label)\n",
    "        \n",
    "    \"\"\"Normalize\"\"\"\n",
    "    mean, std = train_data.float().mean(), train_data.float().std()\n",
    "    train_data = ((train_data - mean) / std, train_label)\n",
    "    valid_data = ((valid_data - mean) / std , valid_label)\n",
    "    \n",
    "\n",
    "    \"\"\"Data For Test\"\"\"\n",
    "    with open('./data/MNIST_pair/test.pkl', 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    test_label = test_data[1]\n",
    "    \n",
    "    \"\"\"Normalize\"\"\"\n",
    "    mean, std = test_data[0].float().mean(), test_data[0].float().std()\n",
    "    test_data = ((test_data[0] - mean) / std, test_label)\n",
    "\n",
    "    \n",
    "    \"\"\"Make Dataset\"\"\"\n",
    "    train_dataset = MNISTDataset(train_data, test=False)\n",
    "    val_dataset = MNISTDataset(valid_data, test=False)\n",
    "    test_dataset = MNISTDataset(test_data, test=True)\n",
    "    \n",
    "    \"\"\"Make DataLoader\"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11edeb8b",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afed0ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, target):\n",
    "    pred_num = torch.argmax(pred, dim=-1)\n",
    "    return (pred_num == target).float().mean()\n",
    "\n",
    "def accuracy_sum(pred1,pred2, target):\n",
    "    pred_num1 = torch.argmax(pred1, dim=-1)\n",
    "    pred_num2 = torch.argmax(pred2,dim=-1)\n",
    "    pred_sum = pred_num1 + pred_num2\n",
    "    return (pred_sum == target).float().mean()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf10549",
   "metadata": {},
   "source": [
    "# Prepare loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da2dbef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_function(predict_method):\n",
    "    if predict_method == \"combination\":\n",
    "        \"\"\"  Make label dictionary storing all permutations corresponding to summation\"\"\"\n",
    "\n",
    "        num = list(range(10))   \n",
    "        num = num * 2\n",
    "        per_list = sorted((set(itertools.permutations(num, 2, ))), key=lambda x: x)\n",
    "\n",
    "        assert len(per_list) == 100\n",
    "\n",
    "        label_dic = defaultdict(list)\n",
    "        for i in per_list:\n",
    "            label_dic[sum(i)].append(i)\n",
    "\n",
    "        label_dic = pd.DataFrame(data=label_dic.values(), index=label_dic.keys()).T\n",
    "\n",
    "        assert 100 == (~label_dic.isna()).sum().sum()\n",
    "\n",
    "        def custom_loss(pred, label):\n",
    "\n",
    "            return nn.NLLLoss(reduction=\"none\")(nn.LogSoftmax(dim=-1)(pred), label)\n",
    "\n",
    "        loss_function = custom_loss\n",
    "        \n",
    "        return loss_function, label_dic\n",
    "    else: \n",
    "        loss_function = nn.CrossEntropyLoss()\n",
    "    \n",
    "        return loss_function, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d952c1db",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d509d01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNIST_Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 6, 5),\n",
    "            nn.BatchNorm2d(6),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2), # 6 24 24 -> 6 12 12\n",
    "            nn.Conv2d(6, 16, 5), # 6 12 12 -> 16 8 8\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2), # 16 8 8 -> 16 4 4\n",
    "        )\n",
    "        \n",
    "        for m in self.encoder:\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab0c09e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, N=10):\n",
    "        super().__init__()\n",
    "        self.classifier =  nn.Sequential(\n",
    "            nn.Linear(16 * 4 * 4, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, N)\n",
    "        )\n",
    "        \n",
    "        for m in self.classifier:\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd5c1c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(predict_method):\n",
    "    encoder = MNIST_Encoder()\n",
    "    \n",
    "    if predict_method == \"combination\":\n",
    "        classifier = Classifier(N=10)\n",
    "    \n",
    "    elif predict_method in ['add_hs', 'add_logits']:\n",
    "        classifier = Classifier(N=19)\n",
    "    else:\n",
    "        raise ValueError (f\"Method only have 3 options : ['add_hs', 'add_logits', 'combination'], but {method} is given\")\n",
    "        \n",
    "    return encoder, classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbfb4ee",
   "metadata": {},
   "source": [
    "## Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c9d13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss_acc(h1, h2, label, classifier, loss_function, label_dic, predict_method: str = \"add_hs\", test=False):\n",
    "    if predict_method == \"add_hs\":\n",
    "        if test:\n",
    "            label = label *2\n",
    "        hidden = (h1 + h2) / 2\n",
    "        \n",
    "        logit = classifier(hidden)\n",
    "        \n",
    "        loss = loss_function(logit, label)\n",
    "\n",
    "        acc = accuracy(logit, label)        \n",
    "    \n",
    "        return loss, acc\n",
    "    \n",
    "    elif predict_method == \"add_logits\":\n",
    "        if test:\n",
    "            label = label *2\n",
    "        \n",
    "        logit1 = classifier(h1)\n",
    "        logit2 = classifier(h2)\n",
    "        \n",
    "        logit = (logit1 + logit2) / 2\n",
    "        \n",
    "        loss = loss_function(logit, label)\n",
    "\n",
    "        acc = accuracy(logit, label)        \n",
    "    \n",
    "        return loss, acc\n",
    "        \n",
    "    elif predict_method == \"combination\":\n",
    "        \n",
    "        if test:\n",
    "            logit = classifier(h1)\n",
    "            \n",
    "            loss = loss_function(logit, label).mean()\n",
    "            \n",
    "            acc = accuracy(logit, label)\n",
    "            \n",
    "            return loss, acc\n",
    "        \n",
    "        logit1 = classifier(h1)\n",
    "        logit2 = classifier(h2)\n",
    "        \n",
    "        loss, acc = 0, 0\n",
    "        \n",
    "        assert label_dic is not None\n",
    "        \n",
    "        for lo1, lo2, (i, combinations) in zip(logit1, logit2, label_dic[label].items()):\n",
    "\n",
    "            label1 = torch.tensor(list(zip(*combinations.dropna().values))[0])\n",
    "            label2 = torch.tensor(list(zip(*combinations.dropna().values))[1])\n",
    "            \n",
    "            lo1 = lo1.expand(label1.shape[0], -1)\n",
    "            lo2 = lo2.expand(label2.shape[0], -1)\n",
    "            \n",
    "            loss += (loss_function(lo1, label1) + loss_function(lo2, label2)).mean()\n",
    "            \n",
    "            acc += (accuracy(lo1, label1) + accuracy(lo2, label2)).mean()\n",
    "            \n",
    "        loss /= len(label)\n",
    "        acc /= len(label)\n",
    "        \n",
    "        return loss, acc\n",
    "        \n",
    "    else:\n",
    "        raise ValueError (f\"Method only have 3 options : ['add_hs', 'add_logits', 'combination'], but {predict_method} is given\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e7f1bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch: int, \n",
    "                encoder: nn.Module,\n",
    "                classifier:nn.Module,\n",
    "                dataloader:torch.utils.data.DataLoader, \n",
    "                loss_function: nn.Module, \n",
    "                optimizer: torch.optim.Optimizer, \n",
    "                lr_scheduler,\n",
    "                predict_method: str,\n",
    "                label_dic = None,\n",
    "               ):\n",
    "    \n",
    "    wandb.define_metric(\"Train/step\")\n",
    "    wandb.define_metric(\"Train/*\", step_metric=\"Train/step\")\n",
    "    \n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "    \n",
    "    with tqdm(enumerate(dataloader), desc=f\"Training Epoch {epoch}\", total=len(dataloader)) as train_bar:\n",
    "        for tri, batch  in train_bar:\n",
    "            \n",
    "            encoder.train()\n",
    "            classifier.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            num1, num2, label = batch\n",
    "\n",
    "            h1 = encoder(num1)\n",
    "            h2 = encoder(num2)\n",
    "\n",
    "            tr_loss, tr_acc = calculate_loss_acc(h1, h2, label, \n",
    "                                                classifier, \n",
    "                                                loss_function, \n",
    "                                                label_dic, \n",
    "                                                predict_method)\n",
    "            \n",
    "            tr_loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            total_loss += tr_loss.item()\n",
    "            total_acc += tr_acc.item()\n",
    "            \n",
    "            train_bar.set_description(f\"Train Step {tri} || Train ACC {tr_acc: .4f} | Train Loss {tr_loss.item(): .4f}\")\n",
    "            \n",
    "            log_dict = {\"Train/step\": tri + epoch*len(dataloader),\n",
    "                        \"Train/Accuracy\": tr_acc,\n",
    "                        \"Train/Loss\": tr_loss}\n",
    "                \n",
    "            wandb.log(log_dict)\n",
    "            \n",
    "    return total_loss/len(dataloader), total_acc / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a8f791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_epoch(epoch: int, \n",
    "                encoder: nn.Module,\n",
    "                classifier:nn.Module,\n",
    "                dataloader:torch.utils.data.DataLoader, \n",
    "                loss_function: nn.Module, \n",
    "                predict_method: str,\n",
    "                label_dic = None                \n",
    "               ):\n",
    "    \n",
    "    wandb.define_metric(\"Valid/step\")\n",
    "    wandb.define_metric(\"Valid/*\", step_metric=\"Valid/step\")\n",
    "        \n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with tqdm(enumerate(dataloader), desc=f\"Val Epoch {epoch}\", total=len(dataloader)) as val_bar:\n",
    "            for vli, batch  in val_bar:\n",
    "\n",
    "                num1, num2, label = batch\n",
    "\n",
    "                h1 = encoder(num1)\n",
    "                h2 = encoder(num2)\n",
    "\n",
    "                vl_loss, vl_acc = calculate_loss_acc(h1, h2, label,\n",
    "                                                    classifier, \n",
    "                                                    loss_function, \n",
    "                                                    label_dic, \n",
    "                                                    predict_method)\n",
    "\n",
    "                total_loss += vl_loss.item()\n",
    "                total_acc += vl_acc.item()\n",
    "\n",
    "                val_bar.set_description(f\"Val Step {vli} || Val ACC {vl_acc: .4f} | Val Loss {vl_loss: .4f}\")\n",
    "                \n",
    "                log_dict = {\"Valid/step\": vli + epoch*len(dataloader),\n",
    "                            \"Valid/Accuracy\": vl_acc,\n",
    "                            \"Valid/Loss\": vl_loss}\n",
    "                \n",
    "                wandb.log(log_dict)\n",
    "                \n",
    "    return total_loss/len(dataloader), total_acc / len(dataloader)               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7be44f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(encoder: nn.Module,\n",
    "        classifier:nn.Module,\n",
    "        dataloader:torch.utils.data.DataLoader, \n",
    "        loss_function: nn.Module,\n",
    "        predict_method: str,\n",
    "        label_dic = None                \n",
    "    ):\n",
    "    \n",
    "    wandb.define_metric(\"Test/step\")\n",
    "    wandb.define_metric(\"Test/*\", step_metric=\"Test/step\")\n",
    "        \n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(enumerate(dataloader), desc=\"Test\", total=len(dataloader)) as test_bar:\n",
    "            for tti, batch  in test_bar:\n",
    "\n",
    "                num, _, label = batch\n",
    "\n",
    "                hs = encoder(num)\n",
    "                \n",
    "                tt_loss, tt_acc = calculate_loss_acc(hs, hs, label,\n",
    "                                                    classifier, \n",
    "                                                    loss_function, \n",
    "                                                    label_dic, \n",
    "                                                    predict_method,\n",
    "                                                    True)\n",
    "\n",
    "                total_loss += tt_loss.item()\n",
    "                total_acc += tt_acc.item()\n",
    "                \n",
    "                test_bar.set_description(f\"Test Step {tti} || Test ACC {tt_acc: .4f} | Test Loss {tt_loss: .4f}\")\n",
    "                \n",
    "                log_dict = {\"Test/step\": tti,\n",
    "                            \"Test/Accuracy\": tt_acc,\n",
    "                            \"Test/Loss\": tt_loss}\n",
    "                \n",
    "                wandb.log(log_dict)\n",
    "\n",
    "    return total_loss/len(dataloader), total_acc / len(dataloader) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ec8037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_epoch, batch_size, predict_method, lr, weight_decay):\n",
    "\n",
    "    \n",
    "    train_loader, valid_loader, test_loader = get_loader(batch_size)\n",
    "    \n",
    "    loss_function, label_dic = get_loss_function(predict_method)\n",
    "    \n",
    "    encoder, classifier = get_models(predict_method)\n",
    "\n",
    "    wandb.watch((encoder, classifier))\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(params=[{\"params\":encoder.parameters(), \"params\":classifier.parameters()}],\n",
    "                                           lr=lr, \n",
    "                                           weight_decay=weight_decay\n",
    "                                 )\n",
    "    \n",
    "    lr_scheduler = get_scheduler(\"cosine\", optimizer=optimizer, \n",
    "                                 num_warmup_steps=int(len(train_loader)*num_epoch*0.1),\n",
    "                                 num_training_steps=len(train_loader)*num_epoch\n",
    "                                )\n",
    "                                           \n",
    "    with tqdm(range(num_epoch), desc=\"Total Epoch\", total=num_epoch) as total_bar:\n",
    "    \n",
    "        for epoch in total_bar:\n",
    "            \n",
    "            train_loss, train_acc = train_epoch(epoch, \n",
    "                                                encoder, \n",
    "                                                classifier, \n",
    "                                                train_loader, \n",
    "                                                loss_function, \n",
    "                                                optimizer, \n",
    "                                                lr_scheduler, \n",
    "                                                predict_method,\n",
    "                                                label_dic)\n",
    "            \n",
    "            valid_loss, valid_acc = valid_epoch(epoch, \n",
    "                                                encoder,\n",
    "                                                classifier,\n",
    "                                                valid_loader,\n",
    "                                                loss_function,\n",
    "                                                predict_method,\n",
    "                                                label_dic)\n",
    "            \n",
    "                            \n",
    "            total_bar.set_description(f\"Epoch {epoch} |||| Train ACC {train_acc:.4f} \\\n",
    "                                        Train Epoch Loss {train_loss:.4f} || \\\n",
    "                                        Valid Epoch ACC {valid_acc:.4f} \\\n",
    "                                        Valid Epoch Loss {valid_loss:.4f}\")\n",
    "            \n",
    "            wandb.log({\"Epoch/Epoch\": epoch,\n",
    "                       \"Total_ACC/Train Epoch ACC\": train_acc,\n",
    "                       \"Total_Loss/Train Epoch Loss\": train_loss,\n",
    "                       \"Total_ACC/Valid Epoch ACC \": valid_acc,\n",
    "                       \"Total_Loss/Valid Epoch Loss\": valid_loss,\n",
    "                        })\n",
    "\n",
    "        test_loss, test_acc = test(encoder,\n",
    "                                    classifier,\n",
    "                                    test_loader,\n",
    "                                    loss_function,\n",
    "                                    predict_method,\n",
    "                                    label_dic)\n",
    "        \n",
    "        wandb.log({\"Total_ACC/Test Accuracy\": test_acc,\n",
    "                    \"Total_Loss/Test Loss\": test_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35a2bcb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epoch = 2\n",
    "batch_size = 64\n",
    "predict_method = ['add_hs', 'add_logits', 'combination'][2]    # ['add_hs' or 'add_logits' or 'combination']\n",
    "lr = 0.0001\n",
    "weight_decay = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cbfeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_epoch': 100, 'batch_size': 128, 'predict_method': 'combination', 'lr': 0.0005, 'weight_decay': 1e-05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhacastle12\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.14.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.33<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">combination0.20639394229845998</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/hacastle12/MNIST%20addition\" target=\"_blank\">https://wandb.ai/hacastle12/MNIST%20addition</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/hacastle12/MNIST%20addition/runs/combination0.20639394229845998\" target=\"_blank\">https://wandb.ai/hacastle12/MNIST%20addition/runs/combination0.20639394229845998</a><br/>\n",
       "                Run data is saved locally in <code>/home/jylab_intern001/MNIST/wandb/run-20230323_055020-combination0.20639394229845998</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a10a554a0841749d9d9ca014328d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Total Epoch:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb19f8bdc04420f8a7a7572fd148346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 0:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import json\n",
    "    from random import random\n",
    "    \n",
    "    with open('./mnist/config.json') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    num_epoch = config.get('num_epoch')\n",
    "    batch_size = config.get('batch_size')\n",
    "    predict_method = config.get('predict_method')    # ['add_hs' or 'add_logits' or 'combination']\n",
    "    lr =config.get('lr')\n",
    "    weight_decay = config.get('weight_decay')\n",
    "    \n",
    "    print(config)\n",
    "    \n",
    "    wandb.init(project=\"MNIST addition\", config=config, id=predict_method+str(random()))\n",
    "    \n",
    "    # wandb.config = {\"epoch\": num_epoch, \n",
    "    #                 'batch_size': batch_size, \n",
    "    #                 'learning_rate': lr, \n",
    "    #                 'weight_decay': weight_decay\n",
    "    #                }\n",
    "    \n",
    "    main(num_epoch=num_epoch, batch_size=batch_size, predict_method=predict_method, lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ad5c68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
